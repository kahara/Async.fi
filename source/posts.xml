<posts>
  <post>
    <published>2011-01-10 14:23:00</published>
    <slug>query_string-parsing-in-plain-c</slug>
    <title>QUERY_STRING parsing in plain C</title>
    <category>snippet</category>
    <tags>
      <tag>cgi</tag>
    </tags>
    <body><![CDATA[<p>
As far as I can tell (which, I'll be the first one to admit, doesn't count for that much) this code is so simple that there are no holes that could be exploited.
</p>

<pre>
  char * query = getenv(&quot;QUERY_STRING&quot;);
  char * pair;
  char * key;
  double value;

  if(query &amp;amp;&amp;amp; strlen(query) &gt; 0) {
    pair = strtok(query, &quot;&amp;amp;&quot;);
    while(pair) {
      key = (char *)malloc(strlen(pair)+1);
      sscanf(pair, &quot;%[^=]=%lf&quot;, key, &amp;amp;value;);
      if(!strcmp(key, &quot;lat&quot;)) {
        lat = value;
      } else if(!strcmp(key, &quot;lng&quot;)) {
        lng = value;
      }
      free(key);
      pair = strtok((char *)0, &quot;&amp;amp;&quot;);
    }
  }
</pre> ]]></body>
  </post>
  <post>
    <published>2010-11-21 10:50:00</published>
    <slug>jquery-boids-plugin</slug>
    <title>jQuery Boids (Plugin)</title>
    <category>experimental</category>
    <tags>
      <tag>boids</tag>
      <tag>jquery</tag>
      <tag>swarm</tag>
    </tags>
    <body><![CDATA[<p>From the README:</p>
<p><i>"My first attempt making a jQuery plugin, following
the guidelines at: <a href="http://docs.jquery.com/Plugins/Authoring">http://docs.jquery.com/Plugins/Authoring</a></i></p>
<p><i>Boids code adapted from Javascript Boids by Ben Dowling,
see: <a href="http://www.coderholic.com/javascript-boids/">http://www.coderholic.com/javascript-boids/</a></i></p>
<p><i>If this is bound to the window resize event, then the
jQuery resize event plugin by "Cowboy" Ben Alman
should be used as it throttles the window resize events.
See: <a href="http://benalman.com/projects/jquery-resize-plugin/">http://benalman.com/projects/jquery-resize-plugin/</a>"</i></p>
<p>The plugin uses HTML Canvas to render the Boids, so a modern browser with Canvas support is required for this to work. I tested with Chrome, Safari and Firefox. IE with Excanvas was <span style="font-weight:bold;">painfully</span> slow&hellip;</p>
<p>Code is hosted at GitHub: <a href="https://github.com/kahara/jQuery-Boids">https://github.com/kahara/jQuery-Boids</a></p>
<p>Demo is at: <a href="http://jonikahara.com/lab/jQuery-Boids/test.html">http://jonikahara.com/lab/jQuery-Boids/test.html</a></p>]]></body>
  </post>
  <post>
    <published>2010-04-03 14:40:00</published>
    <slug>arduino-ds18b20-ethernet-shield-pachube-com</slug>
    <title>Arduino, DS18B20, Ethernet Shield, Pachube.Com</title>
    <category>experimental</category>
    <tags>
      <tag>1-wire</tag>
      <tag>arduino</tag>
      <tag>ds18b20</tag>
      <tag>pachube</tag>
    </tags>
    <body><![CDATA[<a href="http://www.pachube.com/feeds/6469"><img style="width: 400px;" src="/media/2011/06/20100403-426jwimgfh1k95kq92gnfy366.jpg" alt="Kotkansaari Sensorium" /></a>

<span style="font-weight: bold;">Update 2:</span> The sensor is now outside, and running on parasitic power.

<span style="font-weight: bold;">Update:</span> The data is now available in a more mobile-friendly web page <a href="http://jonikahara.com/lab/pachube-6469/">here</a>.

Arduino code, based on (i.e. copypasted &amp; modified a little) stuff from <a href="http://www.dial911anddie.com/weblog/2009/12/arduino-ethershield-1wire-temperature-sensor-pachube/">http://www.dial911anddie.com/weblog/2009/12/arduino-ethershield-1wire-temperature-sensor-pachube/</a> is below. Requires the 1-wire library and the Dallas Temperature Control library, both of which can be downloaded from <a href="http://milesburton.com/wiki/index.php?title=Dallas_Temperature_Control_Library#Download">here</a>. Original code utilized DHCP, but I found this to be somewhat unstable and went with a static IP address instead.

<a href="http://www.maxim-ic.com/quick_view2.cfm?qv_pk=2812">DS18B20</a> gets it's power from Arduino, and the data line (that's the center pin) is connected to Arduino pin 8. Data line is pulled up to +5V through a 4k7 resistor, as suggested in Maxim literature. Parasitic power supply was not used as proper voltage was readily available from Arduino. Please note that even though parasitic power is not used, the pull-up resistor is still necessary (see the <a href="http://datasheets.maxim-ic.com/en/ds/DS18B20.pdf">data sheet</a>).

<pre class="prettyprint">
#include &lt;Ethernet.h&gt;
#include &lt;OneWire.h&gt;
#include &lt;DallasTemperature.h&gt;

char PACHUBE_API_STRING[] = "";  // Your API key
int PACHUBE_FEED_ID = 0; // Your feed ID 

// Digital IO port used for one wire interface
int ONE_WIRE_BUS = 8 ;

// Ethernet mac address - this needs to be unique
byte mac[] = { 0xDE, 0xAD, 0xBE, 0xEF, 0xFE, 0xED };

// IP addres of www.pachube.com
byte server[] = { 209,40,205,190 };

// Arduino address
byte ip[] = { 10, 0, 0, 223 };
byte gateway[] = { 10, 0, 0, 2 };

char version[] = "PachubeClient Ver 0.01c";

#define CRLF "rn"

// simple web client to connect to Pachube.com 
Client client(server, 80);

// Setup a oneWire instance to communicate with any OneWire device
OneWire oneWire(ONE_WIRE_BUS);

// Pass our oneWire reference to Dallas Temperature. 
DallasTemperature sensors(&amp;oneWire);

// 1wire device address
DeviceAddress thermometer;

void setup()
{
   // Note: Ethernet shield uses digitial IO pins 10,11,12, and 13   
   Serial.begin(9600);
  
   Serial.println(version);
   Serial.println();
  
   // locate devices on the 1Wire bus
   Serial.print("Locating devices on 1Wire bus...");
   sensors.begin();
   int count = sensors.getDeviceCount();
   Serial.print("Found ");
   Serial.print( count );
   Serial.println(" devices on 1wire bus");

   // select the first sensor   
   for ( int i=0; i&lt;count; i++ )
   {
      if ( sensors.getAddress(thermometer, i) ) 
      {
         Serial.print("1wire device ");
         Serial.print(i);
         Serial.print(" has address: ");
         printAddress(thermometer);
         Serial.println();
      }
      else
      {
         Serial.print("Unable to find address for 1wire device "); 
         Serial.println( i );
      }  
   }
  
   // show the addresses we found on the bus
   Serial.print("Using 1wire device: ");
   printAddress(thermometer);
   Serial.println();

   // set the resolution to 9 bit 
   sensors.setResolution(thermometer, 9);

   Serial.print("Initializing ethernet...");  
   delay(5000);
   Ethernet.begin(mac, ip, gateway);
   delay(5000);
   Serial.println(" done.");
}

void sendData()
{     
   float temp = sensors.getTempC(thermometer);
   //float temp = sensors.getTempF(thermometer);
   Serial.print("Temp=");
   Serial.println(temp);
  
   Serial.println("connecting...");

   if (client.connect()) 
   {
      Serial.println("connected");
      
      client.print(
         "PUT /api/feeds/" );
      client.print(PACHUBE_FEED_ID);
      client.print(".csv HTTP/1.1" CRLF
                   "User-Agent: Fluffy Arduino Ver 0.01" CRLF
                   "Host: www.pachube.com" CRLF 
                   "Accept: */" "*" CRLF  // need to fix this 
                   "X-PachubeApiKey: " );
      client.print(PACHUBE_API_STRING);
      client.print( CRLF 
                    "Content-Length: 5" CRLF
                    "Content-Type: application/x-www-form-urlencoded" CRLF
                    CRLF );
      client.println(temp);
      unsigned long reqTime = millis();
      
      // wait for a response and disconnect 
      while ( millis() &lt; reqTime + 10000) // wait 10 seconds for response  
      {
         if (client.available()) 
         {
            char c = client.read();
            Serial.print(c);
         }

         if (!client.connected()) 
         {
            Serial.println();
            Serial.println("server disconnected");
            break;
         }
      }
      
      Serial.println("client disconnecting");
      Serial.println("");
      client.stop();
   } 
   else 
   {
      Serial.println("connection failed");
   }
}

void printAddress(DeviceAddress deviceAddress)
{
   for (uint8_t i = 0; i &lt; 8; i++)
   {
      if (deviceAddress[i] &lt; 16) Serial.print("0");
      Serial.print(deviceAddress[i], HEX);
   }
}

void loop()
{
   sensors.requestTemperatures(); // Send the command to get temperatures
   sendData();
   delay( ( 5l * 60l * 1000l) - 11000l  ); // wait 5 minutes
}
</pre>
<pre>
</pre>]]></body>
  </post>
  <post>
    <published>2009-01-06 20:33:00</published>
    <slug>visitor-locator-take-two</slug>
    <title>Visitor Locator, Take Two</title>
    <category>snippet</category>
    <tags>
      <tag>google-maps-api</tag>
      <tag>hostip-info</tag>
      <tag>php</tag>
    </tags>
    <body><![CDATA[The <a href="http://jonikahara.com/lab/visitor-locator-v2/">new version</a> that I hacked together stores number of visits per country and shows the totals when a user clicks a countrys' marker. Visits are stored in an <a href="http://www.sqlite.org/">SQLite</a> database, which, as you may know, makes things very easy as there is no server to look after etc. I was thinking of using <a href="http://en.wikipedia.org/wiki/Berkeley_DB">Berkeley DB</a>, because in an app like this, all that SQL is simply unnecessary sugar, but was lazy in the end (as usual).

Update: Added country flags in place of the same default icon for every country (see: <a href="http://code.google.com/apis/maps/documentation/overlays.html#Custom_Icons">Custom Icons</a> section in Google Maps API).

Update 2: Added tooltip-like functionality, which shows country details in a transient window (label) instead of the default info window. See <a href="http://code.toeat.com/gxmarker.html">GxMarker</a> for additional info.

Continuing here from where last nights' script ended. This is just the PHP side of things; Google Maps API examples can be found elsewhere. First we open an SQLite database and create a table for our visitor data if table does not exist:
<pre class="prettyprint">
try {
        $db = new PDO('sqlite:' . $_SERVER['DOCUMENT_ROOT'] . '/../db/visitor-locator.sqlite3');
} catch(PDOException $exception) {
        die($exception-&gt;getMessage());
}

$stmt = $db-&gt;query('SELECT name FROM sqlite_master WHERE type = 'table'');
$result = $stmt-&gt;fetchAll();
if(sizeof($result) == 0) {
        $db-&gt;beginTransaction();
        $db-&gt;exec('CREATE TABLE visits (country TEXT, visits INTEGER, lat TEXT, lng TEXT);');
        $db-&gt;commit();
}
</pre>
Next, check if the country is already in the table and if it is, increment the 'visits' field:
<pre class="prettyprint">
$stmt = $db-&gt;query('SELECT country, visits FROM visits WHERE country = '' . $countryname . ''');
$result = $stmt-&gt;fetch();

if($result['country']) {
        $db-&gt;beginTransaction();
        $stmt = $db-&gt;prepare('UPDATE visits SET visits=:visits, lat=:lat, lng=:lng WHERE country=:country');
        $stmt-&gt;bindParam(':country', $countryname, PDO::PARAM_STR);
        $visits = $result['visits'] + 1;
        $stmt-&gt;bindParam(':visits', $visits, PDO::PARAM_INT);
        $stmt-&gt;bindParam(':lat', $lat, PDO::PARAM_STR);
        $stmt-&gt;bindParam(':lng', $lng, PDO::PARAM_STR);
        $stmt-&gt;execute();
        $db-&gt;commit();
}
</pre>
If country was not in the table, create a row for it:
<pre class="prettyprint">
else {
        $db-&gt;beginTransaction();
        $stmt = $db-&gt;prepare('INSERT INTO visits (country, visits, lat, lng) VALUES (:country, :visits, :lat, :lng)');
        $stmt-&gt;bindParam(':country', $countryname, PDO::PARAM_STR);
        $visits = 1;
        $stmt-&gt;bindParam(':visits', $visits, PDO::PARAM_INT);
        $stmt-&gt;bindParam(':lat', $lat, PDO::PARAM_STR);
        $stmt-&gt;bindParam(':lng', $lng, PDO::PARAM_STR);
        $stmt-&gt;execute();
        $db-&gt;commit();
}
</pre>
And lastly, fetch all rows and form a Javascript array for our client-side script to use:
<pre class="prettyprint">
$result = $db-&gt;query('SELECT country, visits, lat, lng FROM visits');

echo "&lt;script type=\"text/javascript\"&gt;\n";
echo "//&lt;![CDATA[\n";
echo "var tbl_country = []; var tbl_visits = []; var tbl_lat = []; var tbl_lng = []; var count = 0;\n";
foreach($result-&gt;fetchAll() as $row) {
        echo 'tbl_country[count] = \'' . $row['country'] . '\'; ';
        echo 'tbl_visits[count] = \'' . $row['visits'] . '\'; ';
        echo 'tbl_lat[count] = \'' . $row['lat'] . '\'; ';
        echo 'tbl_lng[count] = \'' . $row['lng'] . '\';';
        echo " count++;\n";
}
echo "//]]&gt;\n";
echo "&lt;/script&gt;\n";
</pre>
<pre>
</pre>]]></body>
  </post>
  <post>
    <published>2009-01-04 18:28:00</published>
    <slug>url-fetch-api-minidom-google-app-engine</slug>
    <title>URL Fetch API, MiniDom (Google App Engine)</title>
    <category>snippet</category>
    <tags/>
    <body><![CDATA[<p>
Fetching stuff with the <a href="http://code.google.com/appengine/docs/urlfetch/">URL Fetch API</a> is simple (especially if one has faith that the source is there and it will deliver inside GAE time limits):
</p>
<pre class="prettyprint">
from google.appengine.api import urlfetch
from xml.dom import minidom

def parse(url):
  r = urlfetch.fetch(url)
  if r.status_code == 200:
    return minidom.parseString(r.content)
</pre>
<p>
As is accessing the resulting DOM with <a href="http://docs.python.org/library/xml.dom.minidom.html">MiniDom</a>. Here the source is an Atom feed:
</p>
<pre class="prettyprint">
import time

dom = parse(URL)
for entry in dom.getElementsByTagName('entry'):
  try:
    published = entry.getElementsByTagName('published')[0].firstChild.data
    published = time.strftime('%a, %d %b', time.strptime(published, '%Y-%m-%dT%H:%M:%SZ'))
  except IndexError, ValueError:
    pass
  &hellip;
</pre>]]></body>
  </post>
  <post>
    <published>2008-01-07 21:23:00</published>
    <slug>berkeley-db-xml-python-basics</slug>
    <title>Berkeley DB XML Python basics</title>
    <category>snippet</category>
    <tags>
      <tag>berkely-db-xml</tag>
      <tag>python</tag>
    </tags>
    <body><![CDATA[<p>
In <a href="http://sivuraide.blogspot.com/2006/05/bdb-xml-document-insertion.html">an earlier post</a> a C++ snippet can be found where a DB XML container was created (or opened if already exists) and a document read from <code>stdin</code> was put into that container. That same snippet done in Python is pretty much identical:
</p>
<pre class="prettyprint">
from bsddb3.db import *
from dbxml import *

mgr = XmlManager(DBXML_ALLOW_EXTERNAL_ACCESS)
uc = mgr.createUpdateContext()

try:
        cont = mgr.openContainer("testcontainer.dbxml", DB_CREATE|DBXML_ALLOW_VALIDATION, XmlContainer.WholedocContainer)
        doc = mgr.createDocument()
        input = mgr.createStdInInputStream()
        doc.setContentAsXmlInputStream(input)
        cont.putDocument(doc, uc, DBXML_GEN_NAME)

except XmlException, inst:
        print "XmlException (", inst.ExceptionCode,"): ", inst.What
        if inst.ExceptionCode == DATABASE_ERROR:
                print "Database error code:",inst.DBError

</pre>]]></body>
  </post>
  <post>
    <published>2006-11-03 16:19:00</published>
    <slug>timeline-dhtml-based-ajaxy-widget-for-visualizing-time-based-events</slug>
    <title>Timeline: "DHTML-based AJAXy widget for visualizing time-based events"</title>
    <category>shoutout</category>
    <tags>
      <tag>css</tag>
    </tags>
    <body><![CDATA[Tr&#232;s cool. Src here:

<a href="http://simile.mit.edu/timeline/api/timeline-api.js">http://simile.mit.edu/timeline/api/timeline-api.js</a>.]]></body>
  </post>
  <post>
    <published>2006-08-19 11:49:00</published>
    <slug>pkg-config</slug>
    <title>pkg-config</title>
    <category>snippet</category>
    <tags/>
    <body><![CDATA[<pre class="prettyprint">
CFLAGS_GTK = `pkg-config --cflags gtk+-2.0`
LIBS_GTK = `pkg-config --libs gtk+-2.0`

gcb-test: main.o
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;$(CC) $(COMPILER_FLAGS) -o gcb-test main.o $(LIBS_GTK)

main.o: main.c
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;$(CC) $(COMPILER_FLAGS) -c main.c -o main.o $(CFLAGS_GTK)
</pre>]]></body>
  </post>
  <post>
    <published>2006-07-01 15:12:00</published>
    <slug>sqlite-preparestepfinalize</slug>
    <title>SQLite, prepare/step/finalize</title>
    <category>snippet</category>
    <tags>
      <tag>sqlite</tag>
    </tags>
    <body><![CDATA[<pre class="prettyprint">
rc =sqlite3_prepare( Db, qry_getnewid, -1, &amp;stmt;, NULL);
if( SQLITE_OK !=rc)
  diediedie( "...");

rc =sqlite3_step( stmt);
if( SQLITE_ROW !=rc)
  {
    sqlite3_finalize( stmt);
    diediedie( "...");
  }

...

a-&gt;id =sqlite3_column_int( stmt, 0);

...

sqlite3_finalize( stmt);
</pre>]]></body>
  </post>
  <post>
    <published>2006-07-01 15:04:00</published>
    <slug>sqlite-open-db-and-exec-sql</slug>
    <title>SQLite, open db and exec sql</title>
    <category>snippet</category>
    <tags>
      <tag>sqlite</tag>
    </tags>
    <body><![CDATA[<pre class="prettyprint">
rc =sqlite3_open(DB_FILENAME, &Db;);
if( rc)
  diediedie( "...");

rc =sqlite3_exec( Db, tbldef_accounts, NULL, 0, &zErrMsg;);
if( SQLITE_OK !=rc)
  diediedie( zErrMsg);

...

sqlite3_close( Db);
</pre>]]></body>
  </post>
  <post>
    <published>2011-01-20 22:11:00</published>
    <slug>desktop-safari-w3c-geolocation-api</slug>
    <title>(Desktop) Safari &amp; (W3C) Geolocation API</title>
    <category>experimental</category>
    <tags>
      <tag>geolocation</tag>
      <tag>javascript</tag>
      <tag>safari</tag>
    </tags>
    <body><![CDATA[I don't know why this thing fails in desktop Safari. I'm using <a href="http://www.modernizr.com/">Modernizr</a> (which is bundled with the <span style="font-weight: bold;">highly recommended</span> <a href="http://html5boilerplate.com/">HTML5 Boilerplate</a>) which reports that the Geolocation API is supported but I'm unable to even get any dialog asking the user for permission to geolocate (which is, if I read the <a href="http://dev.w3.org/geo/api/spec-source.html#security">draft</a> correctly,  REQUIRED to be implemented).

Also, it seems that others (quite a few people actually) have had this same problem but I have been unable to find a solution yet. Below is a snippet from the code; in desktop Safari the PositionErrorCallback (the latter cb function) gets called after timeout but no luck with PositionCallback, no matter how long the timeout value is. Other tested browsers work as expected.

Referenced in other places:
<ul>
	<li><a href="http://stackoverflow.com/questions/3686630/geolocation-not-workin-on-safari-5-x-on-windows-7-xp">Geolocation not workin on Safari 5.x on Windows 7/XP (Stack Overflow)</a></li>
	<li><a href="http://stackoverflow.com/questions/3791442/geolocation-in-safari-5">Geolocation in Safari 5 (Stack Overflow)</a></li>
	<li><a href="http://merged.ca/iphone/html5-geolocation">HTML 5 Geolocation with Safari on the iPhone 3.0 OS, Firefox 3.5 and Chrome (demo page, fails with desktop Safari)</a></li>
</ul>
<p style="font-weight: bold;">(Note to self: check if Google Gears, which is still installed, is causing this?)</p>

[javascript]
var position = null;

$(document).ready(function() {

    if(!Modernizr.geolocation) {
        return;
    }

    navigator.geolocation.watchPosition(
        function(pos) {
            position = {};
            position.lat = pos.coords.latitude;
            position.lng = pos.coords.longitude;
            position.allowed = true;
            init();
        },
        function(error) {
            position = {};
            position.allowed = false;
        },
        {
            enableHighAccuracy: false,
            timeout: 10000,
            maximumAge: 86400000
        }
    );

    checkposition();
});

function checkposition()
{
    log(&quot;checkposition()&quot;);
    if(!position) {
        setTimeout(function() {
            checkposition();
        }, 1000);
        return;
    } else {
        if(position.allowed) {
            log(&quot;checkposition(): got position: &quot; + position.lat + &quot;,&quot; + position.lng);
            fetchephemeris();
        } else {
            log(&quot;checkposition(): could not get a position, giving up&quot;);
            $(&quot;#geolocate&quot;).hide();
        }
    }
}
[/javascript] ]]></body>
  </post>
  <post>
    <published>2011-01-22 23:28:45</published>
    <slug>blogging-activities-migrated-to-async-fi</slug>
    <title>Blogging activities migrated to Async.fi</title>
    <category>meta</category>
    <tags>
      <tag>mysql</tag>
      <tag>sqlite</tag>
      <tag>wordpress</tag>
    </tags>
    <body><![CDATA[Installing Wordpress and getting the basic settings straight was, all and all, a relatively painless experience. One downside of choosing this particular blogging platform is that I now have to run a MySQL server just for this purpose (I have no intention to use MySQL for anything else as I have actively tried to steer away from RDBMSs, even before this whole "NoSQL" thing became fashionable). I would've preferred to be able to run Wordpress on top of an SQLite backend, just because that would've kept things one step simpler, but I just could not get the <a href="http://wordpress.org/extend/plugins/pdo-for-wordpress/">"PDO (SQLite) For WordPress"</a> adapter to work with a reasonable amount of work, so I gave up and went ahead with a MySQL server install. So it's a tradeoff (what isn't?) but with all these plugins and stuff and whatnot that this system supports I think it's a pretty good deal.

I went through the old posts at <a href="http://sivuraide.blogspot.com/">sivuraide.blogspot.com</a> and imported what I thought I might find useful at least to some extent. Unsurprisingly not too many of the old posts fall to this category. Then again, the old blog is titled Code <em>Notebook</em>. The old posts did not "just work" in all places, a quick eyeballing revealed some glitches that need to be ironed out.

I installed the following plugins in addition to what was bundled:

<ul>
  <li><a href="http://crowdfavorite.com/wordpress/plugins/wordpress-mobile-edition/">WordPress Mobile Edition</a></li>
  <li><a href="http://yoast.com/wordpress/google-analytics/">Google Analytics for WordPress</a></li>
  <li><a href="http://extrafuture.com/la-petite-url/">la petite url</a></li>
  <li><a href="http://www.beautyorange.com/beauty-orange-projects/beauty-orange-wordpress-code-prettifier/">Beauty Orange WordPress Code Prettifier</a></li>
  <li><a href="http://kingdesk.com/projects/wp-typography/">wp-Typography</a></li>
</ul>
]]></body>
  </post>
  <post>
    <published>2011-02-27 17:07:49</published>
    <slug>http-live-streaming-with-vlc</slug>
    <title>HTTP Live Streaming with VLC</title>
    <category>experimental</category>
    <tags>
      <tag>aac</tag>
      <tag>h264</tag>
      <tag>http-live-streaming</tag>
      <tag>vlc</tag>
    </tags>
    <body><![CDATA[Well, it took "a while" but I finally got <a title="HTTP Live Streaming" href="http://en.wikipedia.org/wiki/HTTP_Live_Streaming">HTTP Live Streaming</a> working with <a title="VLC" href="http://www.videolan.org/vlc/">VLC</a>. Downloading and compiling the latest<span id="hwytop"> </span> from <a title="Videolan's Git repo" href="http://git.videolan.org/?p=vlc.git">Videolan's Git repo</a> was required ("1.2.0-git Twoflower" here). I might add that even though on the box that I did this I've compiled a lot of different programs (an Ubuntu installation that has gone through multiple dist-upgrades so it's a few years old and has a lot of packages (2344 atm) installed), quite a few external <em>-dev</em> packages relating to audio and video had to be apt-get'ed to make things work.

Below is the command to make VLC read a DVD and generate a segmented stream of H264 video and AAC audio to directory /var/www/html-video-stream/x/ on our local web server. In an IRL situation we would perhaps run the transcoder and segmenter instances on separate machines, or if we already had a suitable H264 stream source (like a camera) we could skip the transcoding step altogether.
<pre>vlc -v -I "dummy" dvdsimple:///dev/scd0 &#187;
:sout="#transcode{vcodec=h264,vb=100, &#187;
venc=x264{aud,profile=baseline,level=30,keyint=30,ref=1}, &#187;
aenc=ffmpeg{aac-profile=low},acodec=mp4a,ab=32,channels=1,samplerate=8000} &#187;
:std{access=livehttp{seglen=10,delsegs=true,numsegs=5, &#187;
index=/var/www/html-video-stream/x/stream.m3u8, &#187;
index-url=http://192.168.1.33/html-video-stream/x/stream-########.ts}, &#187;
mux=ts{use-key-frames}, &#187;
dst=/var/www/html-video-stream/x/stream-########.ts}"
</pre>
&nbsp;

QuickTime X (fanboys have had this since Snow Leopard) supports HTTP Live Streaming, so in order to show the above stream on a web page in Safari using the &lt;video&gt; tag, we can do the following:
<pre id="line62">&lt;video autoplay loop controls&gt;
  &lt;source src="http://192.168.1.33/html-video-stream/x/stream.m3u8" &#187;
   type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'/&gt;
  &lt;!-- additional sources here --&gt;
&lt;/video&gt;</pre>
&nbsp;

Although I'm not sure if this will work in a situation where we attempt to feed H264 to clients that don't support HTTP Live Streaming, that is, we have an additional &lt;source&gt; element that points to a "regular" H264 HTTP stream. However, adding Ogg/Theora and WebM/VP8 support should not cause problems &#8211; I just haven't been able to make VLC output those (properly) yet. HTML5 video tag streaming support in different browsers is also one big question mark.]]></body>
  </post>
  <post>
    <published>2011-03-09 21:40:33</published>
    <slug>h264-http-test-stream-generator</slug>
    <title>H264 HTTP Test Stream Generator</title>
    <category>experimental</category>
    <tags>
      <tag>gstreamer</tag>
      <tag>h264</tag>
      <tag>vlc</tag>
    </tags>
    <body><![CDATA[As I haven't got an H264-capable camera to use as a test source (yet) I'm using the following GStreamer pipeline, adapted from <a href="http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-plugins/html/gst-plugins-base-plugins-videotestsrc.html">videotestsrc documentation</a> to generate an endless, mildly hypnotic low bitrate <a href="http://en.wikipedia.org/wiki/Zone_plate">zone plate</a> pattern wrapped in an MPEG transport stream. A clock is also shown so that when the stream is transcoded and/or segmented, it's easy to see how bad the lag is. Audio is not included but for example audiotestsrc could be plugged in the pipeline if necessary (although I won't be using audio in my app). VLC is used in the end of the command line to serve the stream over HTTP.
<pre>
gst-launch-0.10 -v mpegtsmux name="mux" ! fdsink fd=1 &#187;
videotestsrc pattern=zone-plate kx2=20 ky2=20 kt=1 ! &#187;
video/x-raw-yuv,width=320,height=240 ! &#187;
clockoverlay valign=bottom halign=left font-desc="Sans 23" ! &#187;
ffmpegcolorspace ! videorate ! video/x-raw-yuv,framerate=15/1 ! &#187;
x264enc bitrate=100000 cabac=false pass=qual quantizer=27 &#187;
subme=4 threads=0 bframes=0 dct8x8=false ! mux. | &#187;
vlc -I "dummy" file/ts:///dev/stdin &#187;
:sout='#std{access=http{mime=video/mp4},mux=ts,dst=192.168.1.35:8000}'
</pre>]]></body>
  </post>
  <post>
    <published>2011-03-26 20:14:02</published>
    <slug>xenserver-hosting-paravirtual-64-bit-ubuntu-10-04-guests</slug>
    <title>XenServer hosting paravirtual 64-bit Ubuntu 10.04 guests</title>
    <category>shoutout</category>
    <tags>
      <tag>paravirtualization</tag>
      <tag>ubuntu</tag>
      <tag>windows</tag>
      <tag>xen</tag>
    </tags>
    <body><![CDATA[<a rel="lightbox" href="/media/2011/03/xencenter.png"><img src="/media/2011/03/xencenter1.png" alt="XenCenter" title="XenCenter" width="100%" class="alignleft size-full wp-image-477" /></a>

While looking for a virtualization solution in order to make computational matters more flexible, efficient and manageable (et cetera, et cetera) here at home, various offerings that are listed below were tested. To be honest, to say that I "tested" these would be twisting the truth quite a bit as the methodology used was not very scientific and things were guided more by hunch than strict reason. But then again as I would be the only one who would get hurt if things went horribly wrong it wouldn't really matter that much if the "wrong" solution was chosen. So far, after a few days, it's looking like the choice I made was right. The following took part in our non-scientific non-review:

<ul>
	<li><a href="http://open.eucalyptus.com/">Eucalyptus</a> <a href="http://www.ubuntu.com/business/cloud/overview">(Ubuntu Enterprise Cloud)</a></li>
	<li><a href="http://www.parallels.com/eu/products/server/baremetal/">Parallels Server 4 Bare Metal Edition</a></li>
	<li><a href="http://www.vmware.com/products/vsphere-hypervisor/overview.html">VMware vSphere Hypervisor&#8482; (ESXi)</a></li>
	<li><a href="http://www.citrix.com/English/ps2/products/product.asp?contentID=683148">XenServer Free Edition</a></li>
</ul>

At first Eucalyptus sounded like an awesome choise, given for example its Amazon EC2 API compatibility, but in practice it turned out that while the idea of having a private cloud at one's disposal is great, having this much flexibility brings with it a much higher level of complexity in managing the system, which pretty much makes the whole idea of having a cloud a moot point. And as I have just one host machine, running Eucalytus wasn't as straightforward as it could be. And also, what I'm really looking for is virtualization of a couple of servers that I like to have around, not a pool of cloud computing resources which can these days be bought at very reasonable rates (or for <a href="http://aws.amazon.com/free/">free</a>, even) if needed. Nice offering, though, which I bet we will see gaining more and more ground in the future. Oddly, I'm unable to find a single service provider offering a service similar to EC2, but built on top of Eucalyptus. Perhaps the tools that would facilitate selling an Eucalyptus-based cloud service do not yet exist?

As for Parallels, I think it's debatable just how "bare metal" their hypervisor really is. It may be so that I have let myself be enchanted by marketers to believe that this bare metal thing is something radically different. The other possibility is that Parallels themselves are bending the meaning of the term here and are selling their system as "bare metal" when in fact it's not that bare. At least to me it looked like a full host operating system was installed and I can't see how this makes things that much different from having a regular server and running the hypervisor on it. Of course one difference is that you don't need for example a Windows server license to run the software but there's still a regular operating system involved that's running the show. Don't get me wrong, I use Parallels products almost on a daily basis (for example the illustration image on top, of the XenCenter management tool, is running on Windows XP installation inside a Parallels Desktop for Mac virtual machine) and I have nothing against them, it's just that this personal experience I have with their server offering wasn't that super. Their management tools are cross-platform (all three Windows, Mac, Linux) which is a plus but they want $500 for a license per server which I'm not going to pay them. Also, Parallels Server could be considered somewhat obscure in comparison to the others so this may very well turn out to not be a good choice in the long run.

VMWare's offer just wouldn't work for some reason, perhaps my hardware was somehow incompatible. Or something, I don't know. VMWare being such a traditional virtualization house, this would've been the "correct" choice in a similar way to <em>"No manager ever got fired for buying IBM"</em>. But as no one was going to fire me for whatever choice I made here I gave up and moved on.

Last on the table was XenServer from Citrix. I went with the default installation and just used one whole disk and let the installer set things up the default way, i.e. a few gigabytes for all the Xen stuff and the rest for LVM storage. Like rest, the system can be managed from command line (local console or over SSH) but as my primary aim here is to get things done and having a point and click interface makes learning curve that much less steep, I went and installed the XenCenter management console on a Windows XP virtual machine (which was of course not hosted on this machine).

Making a <a href="http://en.wikipedia.org/wiki/Paravirtualization">paravirtual</a> Ubuntu guest did not require any kind of wizardy, I just followed steps 3&ndash;5 in <a href="http://community.citrix.com/display/xs/Installing+Ubuntu+Server+10.04+%2832bit+and+64bit%29+LTS/"><em>Installing Ubuntu Server 10.04 (32bit and 64bit) LTS</em></a> (steps 1 and 2 were not necessary as the Ubuntu 10.04 64-bit template was already there after a fresh install). After I had one machine set up I turned that in to a virtual machine template and using this template it's super-fast to start new servers when needed. Also, these (para)virtual machines don't seem to be taking much of a performance hit and all and all I'm really pleased with the results.

The only thing missing here is <a href="http://www.lm-sensors.org/">lm-sensors</a> or something similar so that I could at least see CPU and motherboard temperatures of a running system but I suppose this can be arranged.

<strong>Update:</strong> A Windows XP guest, with paravirtual device drivers, was also easy enough to <a href="http://docs.vmd.citrix.com/XenServer/5.6.0fp1/1.0/en_gb/guest.html#install_windows">install</a>. And following the instructions in chapter <a href="http://docs.vmd.citrix.com/XenServer/5.6.0fp1/1.0/en_gb/guest.html#clone_considerations_Windows">3.4. "Preparing to clone a Windows VM"</a>, working with Redmond is greatly simplified as an XP template can be prepared and new virtual machines invoked on demand and disposed of after use &ndash; this way one doesn't have to worry if for example installing a software package for testing purposes will mess the system up somehow.]]></body>
  </post>
  <post>
    <published>2011-05-08 10:09:45</published>
    <slug>building-a-wds-bridge-with-consumer-grade-wlan-aps</slug>
    <title>Building a WDS Bridge with Consumer Grade WLAN APs</title>
    <category>experimental</category>
    <tags>
      <tag>stp</tag>
      <tag>wds</tag>
      <tag>wlan</tag>
    </tags>
    <body><![CDATA[<a href="/media/2011/04/a-link.jpg"><img class="alignleft size-full wp-image-543" title="a-link" src="/media/2011/04/a-link.jpg" alt="" width="100%" /></a>

Small AP is small &#8211; and has a built-in antenna, too. I got two of <a href="http://store.a-link.com/fi/WNA.html">these</a> (for 19,90&#8364; per piece &#8211; not A-link list price&#8230;) and set up a bridge so I could relocate my noisy <a href="https://async.fi/2011/03/xenserver-hosting-paravirtual-64-bit-ubuntu-10-04-guests/">Xen box</a> from living room to kitchen to keep the box running 24/7 <em>and</em> sleep. (Turns out that in the end even this didn't help because the box remained loud enough to disturb sleep no matter what settings were selected in BIOS thermal management.) Initially it looked like the bridge worked just fine, except my testing revealed that the transmission speed was nowhere near the advertised "IEEE 802.11n (draft 2.0) / 150Mb":

<pre>
XenCenter.iso              100%   44MB   1.5MB/s   00:29
</pre>

After trying different cryptos from WPA2 to plain text and fiddling with various other settings I came to the conclusion that the slow speed was a <em>feature</em> of the device. Anyway, this was not really any kind of concern as I was more interested in latency, which was low enough (a few milliseconds). Put all this together and my opinion is that it's good enough for an access point that is about the size of a deck of cards and costs twenty euros.

What did turn out to be a problem is that at times the AP's would somehow manage get a broadcast storm going on, which of course  took the wired network down with it very quickly. I wasn't really able to get to the root of this but from what I observed I can tell that the broadcast storm would happen even when one AP was connected to the primary wired segment and the AP at the other end was just "floating" there, with nothing connected to its' Ethernet ports. Also, while after enabling STP in the devices I could, using <code>tcpdump</code>, observe the STP config packets doing their thing and reconfiguring after for example dropping and then reconnecting either end of the bridge, this (STP) did nothing to prevent the broadcast storm from happening. I should also note for the record that I was using the "WDS", not "AP+WDS" mode. 

Verdict: the devices just aren't suitable for this application, i.e. they are buggy and do not fully work as advertised but given their relatively compact size and ability to function as clients on a WLAN, I'll keep these.]]></body>
  </post>
  <post>
    <published>2011-05-08 18:26:23</published>
    <slug>windows-server-2008-r2-on-amazon-ec2</slug>
    <title>Windows Server 2008 R2 on Amazon EC2</title>
    <category>experimental</category>
    <tags>
      <tag>aws</tag>
      <tag>ebs</tag>
      <tag>ec2</tag>
      <tag>pricing</tag>
      <tag>remote-desktop-services</tag>
      <tag>windows-server-2008-r2</tag>
    </tags>
    <body><![CDATA[<a href="/media/2011/05/20110508-xdjdyit8w4hg3dbicwu8gged68.png"><img class="alignleft size-full wp-image-641" title="Windows Server 2008 R2 on Amazon EC2 Micro Instance" src="/media/2011/05/20110508-xdjdyit8w4hg3dbicwu8gged68.png" alt="" width="1440" height="937" /></a>

Plan to use an in-house box to run a <a href="https://async.fi/2011/03/xenserver-hosting-paravirtual-64-bit-ubuntu-10-04-guests/">XenServer</a> to host XP instances (I need multiple Windows desktops for "testing" purposes if anyone asks) had to be scrapped because the box was simply too loud and I couldn't get the <a href="https://async.fi/2011/05/building-a-wds-bridge-with-consumer-grade-wlan-aps/">wireless bridge to work</a> &#8211; not that the latter would have helped anyway because like I said the box really is loud and relocating it anywhere inside our flat just wouldn't lower the noise level enough for it to not disturb sleep.

Which brings us here: launching a Windows Server 2008 R2 instance on Amazon EC2 and setting up Remote Desktop Services to enable multiple simultaneous client sessions. Below we can see Alice, Bob, Charlie and Dave each happily running their own Remote Desktop session at the same time:

<a href="/media/2011/05/20110508-8d6fgppd1h5ejehpqun8cu9hhu.png"><img class="alignleft size-full wp-image-656" title="Alice, Bob, Charlie and Dave" src="/media/2011/05/20110508-8d6fgppd1h5ejehpqun8cu9hhu.png" alt="" width="1678" height="1049" /></a>

&nbsp;

The whole thing runs "tolerably" smoothly even on the severely memory-limited Micro Instace:

<a href="/media/2011/05/20110508-dmhedmbyncijb81pkqu12d2d19.png"><img class="aligncenter size-full wp-image-663" title="Taskmgr" src="/media/2011/05/20110508-dmhedmbyncijb81pkqu12d2d19.png" alt="" width="403" height="446" /></a>

At $0.035 per hour this can be considered cheap. And, the server can be shut down when it's not needed in which case the only charge will be for the admittedly humonguos (35 gigabytes) Windows root partition. And of course those clients would need Client Access Licenses which adds a one time cost of roughly $100 per client. Now, to directly compare this kind of setup with having an actual physical server would indicate poor judgement as both have their strong and weak points but costs can be compared. So here we have an estimate of what the total cost of running a server like this for a three-year period would be, <em>sans</em> CALs:
<table>
<tbody>
<tr>
<th></th>
<th>On-Demand EC2</th>
<th>Reserved EC2
(1-year Contract)</th>
<th>Reserved EC2
(3-year Contract)</th>
</tr>
<tr>
<td>One-time costs</td>
<td>$0.00</td>
<td>$54.00</td>
<td>$82.00</td>
</tr>
<tr>
<td>Compute</td>
<td>$922.32</td>
<td>$421.56</td>
<td>$421.56</td>
</tr>
<tr>
<td>Storage (35 GB)</td>
<td>$138.60</td>
<td>$138.60</td>
<td>$138.60</td>
</tr>
<tr>
<td>I/O (10 IOPS)</td>
<td>$103.00</td>
<td>$103.00</td>
<td>$103.00</td>
</tr>
<tr>
<td>Transfer In (1 GB/m)</td>
<td>$3.60</td>
<td>$3.60</td>
<td>$3.60</td>
</tr>
<tr>
<td>Transfer Out (10 GB/m)</td>
<td>$48.60</td>
<td>$48.60</td>
<td>$48.60</td>
</tr>
<tr>
<td>Total Cost (Euros)</td>
<td><strong>849.69 &#8364;</strong></td>
<td><strong>613.00 &#8364;</strong></td>
<td><strong>557.11 &#8364;</strong></td>
</tr>
<tr>
<td>Per Month (Euros)</td>
<td><strong>23.60 &#8364;</strong></td>
<td><strong>17.03 &#8364;</strong></td>
<td><strong>15.48 &#8364;</strong></td>
</tr>
</tbody>
</table>
Source: <a href="http://calculator.s3.amazonaws.com/calc5.html">http://calculator.s3.amazonaws.com/calc5.html</a>

Then again, that 600 &#8364; would get you two HP Proliant MicroServers. Yet, then again, that price does not include Windows licenses and they would need a physical location, electricity, an Internet connection &#8211; an so on.]]></body>
  </post>
  <post>
    <published>2011-05-14 14:21:01</published>
    <slug>verifying-amazon-sns-messages-with-php</slug>
    <title>Verifying Amazon SNS Messages with PHP</title>
    <category>snippet</category>
    <tags>
      <tag>aws</tag>
      <tag>php</tag>
      <tag>sns</tag>
      <tag>x-509</tag>
    </tags>
    <body><![CDATA[Messages sent by <a href="http://aws.amazon.com/sns/">Amazon Simple Notification Service</a> are signed, and checking that any received message is indeed from AWS and not from some douche trying to outsmart you is not very hard (nor should it be optional, for that matter):

<a href="/media/2011/05/sns-verify.php_1.txt">sns-verify.php</a>

The <code>verify_sns()</code> function expects the message in JSON format, plus region (e.g. "eu-west-1"), numerical account ID without dashes and an array containing the topics you're interested in. The code will verify both <code>SubscriptionConfirmation</code> and <code>Notification</code> messages. It loads the certificate from the address in <code>SigningCertURL</code> field  to check against for each message separately because the certificate changes over time, as described <a href="https://forums.aws.amazon.com/ann.jspa?annID=882">here</a>. It is also checked that the host where the certificate is loaded from is in the amazonaws.com domain.

Example usage where subscriptions are automatically confirmed:

<pre>
require_once('sns-verify.php');

if($_SERVER['REQUEST_METHOD'] != 'POST') {
    logger('Not POST request, quitting');
    exit();
}

$post = file_get_contents('php://input');

if(!verify_sns($post, 'REGION', 'ACCOUNT', array('TOPIC 1', 'TOPIC 2'))) {
    exit;
}

$msg = json_decode($post);


if($msg->Type == 'SubscriptionConfirmation') {
    logger('SNS SubscriptionConfirmation received);
    file_get_contents($msg->SubscribeURL);
} elseif($msg->Type == 'Notification') {
    logger('SNS Notification received);
    process_message($msg);
}
</pre>]]></body>
  </post>
  <post>
    <published>0000-00-00 00:00:00</published>
    <slug/>
    <title>Cliptogif.com minutiae &amp; miscellanea</title>
    <category>note</category>
    <tags>
      <tag>cliptogif-com</tag>
      <tag>ffmpeg</tag>
    </tags>
    <body><![CDATA[<p>
Talking about <a href="http://cliptogif.com">this</a>.
</p>

<ol>

<li>
<p>
To extract a sequence of still frames from an input video file, keeping frame width and height below 500px (scaling down as necessary):
</p>
<pre>
ffmpeg -i input.mov -vf scale='min(512\, iw):-1' -vf scale='-1:min(512\, ih)' output/`date +%s`-%05d.jpg
</pre>
<p>
Do note that stock ffmpeg shipped with Debian 6 does not support video filters (<code>-vf</code> option); I solved this by compiling from source.
</p>
</li>

</ol>]]></body>
  </post>
  <post>
    <published>0000-00-00 00:00:00</published>
    <slug/>
    <title>Javascript namespacing</title>
    <category>snippet</category>
    <tags>
      <tag>javascript</tag>
      <tag>namespace</tag>
    </tags>
    <body><![CDATA[Following advice given in <a href="http://enterprisejquery.com/2010/10/how-good-c-habits-can-encourage-bad-javascript-habits-part-1/">this</a> post by "that Elijah guy", i.e. <a href="https://plus.google.com/110944993173615692737/posts">Elijah Manor</a>:

[javascript]

[/javascript] ]]></body>
  </post>
  <post>
    <published>2011-05-29 14:33:04</published>
    <slug>reading-ec2-tags-with-boto</slug>
    <title>Reading EC2 tags with Boto</title>
    <category>snippet</category>
    <tags>
      <tag>aws</tag>
      <tag>boto</tag>
      <tag>django</tag>
      <tag>ec2</tag>
      <tag>python</tag>
    </tags>
    <body><![CDATA[<strong>(Ouch! Looks like WordPress update to 3.1.3 wiped all the modifications I made to the default theme. Admittedly I should've seen that coming.)</strong>

What I want to do is basically attach a key-value pair to an EC2 instance when launching it in AWS Management Console and read the value inside the instance when it's running. To be more specific, I use this to to set a key called <code>environment</code> that can have values like <code>dev</code>, <code>stage</code> and <code>prod</code> so that the Django config can decide which database to connect to etc. while starting up. I suspect that in Boto the current instance can somehow be referenced in a more direct fashion but this works as well.

First, append the following to <code>/etc/profile</code>:
<pre># See: http://stackoverflow.com/questions/625644/find-out-the-instance-id-from-within-an-ec2-machine
export EC2_INSTANCE_ID="`wget -q -O - http://169.254.169.254/latest/meta-data/instance-id || die \"wget instance-id has failed: $?\"`"
test -n "$EC2_INSTANCE_ID" || die 'cannot obtain instance-id'
export EC2_AVAIL_ZONE="`wget -q -O - http://169.254.169.254/latest/meta-data/placement/availability-zone || die \"wget availability-zone has failed: $?\"`"
test -n "$EC2_AVAIL_ZONE" || die 'cannot obtain availability-zone'
export EC2_REGION="`echo \"$EC2_AVAIL_ZONE\" | sed -e 's:\\([0-9][0-9]*\\)[a-z]*\\$:\\\\1:'`"</pre>
Now we know the region and instance ID. Next, install <a href="http://boto.cloudhackers.com/">Boto</a> by running the following commands:
<pre>wget "http://boto.googlecode.com/files/boto-2.0b4.tar.gz"
zcat boto-2.0b4.tar.gz | tar xfv -
cd boto-2.0b4
python ./setup.py install</pre>
Then, add these lines to <code>~/.profile</code>:
<pre>export AWS_ACCESS_KEY_ID=&lt;ACCESS_KEY&gt;
export AWS_SECRET_ACCESS_KEY=&lt;SECRET_KEY&gt;</pre>
Or the equivalent in <code>~/.boto</code>:
<pre>[Credentials]
aws_access_key_id = &lt;ACCESS_KEY&gt;
aws_secret_access_key = &lt;SECRET_KEY&gt;</pre>
Now, to read the tag we want in Python:
<pre>#!/usr/bin/env python                                                                                                                                           

import os
from boto import ec2

ec2_instance_id = os.environ.get('EC2_INSTANCE_ID')
ec2_region = os.environ.get('EC2_REGION')

conn = ec2.connect_to_region(ec2_region)

reservations = conn.get_all_instances()
instances = [i for r in reservations for i in r.instances]

for instance in instances:
    if instance.__dict__['id'] == ec2_instance_id:
        print instance.__dict__['tags']['environment']</pre>]]></body>
  </post>
  <post>
    <published>2011-06-21 16:05:39</published>
    <slug>autofont-py</slug>
    <title>autofont.py</title>
    <category>snippet</category>
    <tags>
      <tag>css</tag>
      <tag>font-size</tag>
    </tags>
    <body><![CDATA[It ain't pretty but it works. Interpolator function is from: <a href="http://www.zovirl.com/2008/11/04/interpolated-lookup-tables-in-python/"><em>Interpolated Lookup Tables in Python</em></a>. The idea here is to generate tables like this so that text scales smoothly based on viewport width:

<pre>
@media screen and (max-width: 480px) { body { font-size: 0.500000em; } }
@media screen and (min-width: 481px) and (max-width: 720px) { body { font-size: 0.750em; } }
@media screen and (min-width: 721px) and (max-width: 960px) { body { font-size: 1.000em; } }
@media screen and (min-width: 961px) and (max-width: 1200px) { body { font-size: 1.250em; } }
@media screen and (min-width: 1201px) and (max-width: 1440px) { body { font-size: 1.500em; } }
@media screen and (min-width: 1441px) and (max-width: 1680px) { body { font-size: 1.750em; } }
@media screen and (min-width: 1921px) { body { font-size: 2.000000em; } }
</pre>
Here's the script:
<pre>
import sys

if len(sys.argv) &lt; 7:
    print 'usage: %s LOWEST_RESOLUTION HIGHEST_RESOLUTION SMALLEST_FONTSIZE LARGEST_FONTSIZE STEPS FONT_UNIT' % (sys.argv[0])
    sys.exit(1)

llowest_resolution = int(sys.argv[1])
highest_resolution = int(sys.argv[2])
smallest_fontsize = float(sys.argv[3])
largest_fontsize = float(sys.argv[4])
steps = int(sys.argv[5])-1
font_unit = str(sys.argv[6])

resolutions = InterpolatedArray(((1, lowest_resolution), (steps, highest_resolution)))
fontsizes = InterpolatedArray(((1, smallest_fontsize), (steps, largest_fontsize)))

print '@media screen and (max-width: %dpx) { body { font-size: %f%s; } }' % (lowest_resolution, smallest_fontsize, font_unit)

for i in range(2, steps+1):
    print '@media screen and (min-width: %dpx) and (max-width: %dpx) { body { font-size: %.3f%s; } }' % (resolutions[i-1], resolutions[i], fontsizes[i], font_unit)

print '@media screen and (min-width: %dpx) { body { font-size: %f%s; } }' % (highest_resolution, largest_fontsize, font_unit)</pre>

<p>
Note: to make this work in Internet Explorer versions &lt; 9, include the <a href="http://code.google.com/p/css3-mediaqueries-js/">css3-mediaqueries.js script</a> by Wouter van der Graaf.
</p>
]]></body>
  </post>
  <post>
    <published>2011-06-30 05:29:40</published>
    <slug>emacs-and-utf-8-encoding</slug>
    <title>"Emacs and UTF-8 Encoding"</title>
    <category>snippet</category>
    <tags>
      <tag>emacs</tag>
      <tag>python</tag>
      <tag>unicode</tag>
    </tags>
    <body><![CDATA[From <a href="http://blog.jonnay.net/archives/820-Emacs-and-UTF-8-Encoding.html">http://blog.jonnay.net/archives/820-Emacs-and-UTF-8-Encoding.html</a>:
<pre>;;;;;;;;;;;;;;;;;;;;
;; set up unicode
(prefer-coding-system       'utf-8)
(set-default-coding-systems 'utf-8)
(set-terminal-coding-system 'utf-8)
(set-keyboard-coding-system 'utf-8)
;; This from a japanese individual.  I hope it works.
(setq default-buffer-file-coding-system 'utf-8)
;; From Emacs wiki
(setq x-select-request-type '(UTF8_STRING COMPOUND_TEXT TEXT STRING))
;; MS Windows clipboard is UTF-16LE
(set-clipboard-coding-system 'utf-16le-dos)</pre>
Also, add this to the beginning of your source files when working with Python (otherwise you'll get "<code>SyntaxError: Non-ASCII character '\xc3' in file&#8230;</code>" etc. errors):
<pre># -*- coding: utf-8 -*-</pre>]]></body>
  </post>
  <post>
    <published>2011-07-15 09:56:24</published>
    <slug>note-to-self-djangos-error-cannot-import-name-foo</slug>
    <title>Note to self: Django's "Error: cannot import name Foo"</title>
    <category>note</category>
    <tags>
      <tag>django</tag>
    </tags>
    <body><![CDATA[If Foo is definitely defined, then this is most likely a circular import which can be avoided by dropping the "redundant" import statement and referring not directly to the Model itself, but its' <em>name</em>. This is known as a lazy relationship:

<pre>foo = models.ForeignKey('Foo')</pre>]]></body>
  </post>
  <post>
    <published>2011-07-16 08:12:22</published>
    <slug>note-to-self-djangosouth-basic-usage</slug>
    <title>Note to self: Django/South basic usage</title>
    <category>note</category>
    <tags>
      <tag>database</tag>
      <tag>django</tag>
      <tag>migration</tag>
      <tag>south</tag>
    </tags>
    <body><![CDATA[Not that I understand how this schema migration thing actually works. Anyway, initially, on an empty database:

<pre>
./manage.py syncdb --noinput
./manage.py schemamigration YOUR_APP --initial
./manage.py migrate YOUR_APP 0001 --fake
</pre>

Then, after fiddling with your models:
<pre>
./manage.py schemamigration YOUR_APP --auto
./manage.py migrate YOUR_APP
</pre>

That should do it. But like I said, my understanding of the system is very limited and I'm sure there are cases when this simplistic pattern just won't cut it. My needs, however, at the moment are not the most complicated.]]></body>
  </post>
  <post>
    <published>2011-07-23 19:21:21</published>
    <slug>backbone-js-automagic-syncing-of-collections-and-models</slug>
    <title>Backbone.js "automagic" syncing of Collections and Models</title>
    <category>snippet</category>
    <tags>
      <tag>ajax</tag>
      <tag>backbone-js</tag>
      <tag>tastypie</tag>
    </tags>
    <body><![CDATA[
    <p>The idea here is to periodically <code>fetch()</code> and keep the client and server Collections in sync in such a way that the consuming View(s) only get updated when a Model is added to or removed from a Collection, or the attributes of one of the Models in it change. What's done is:
    </p>
<ul>
	<li>Compare the existing Collection to the incoming set and remove every Model from the existing Collection that is not in the incoming set.</li>
	<li>Compare the incoming set to the existing Collection and add every Model from the incoming set to the existing Collection that is not already there.</li>
	<li>Compare each Model in the two sets and update the ones in the existing Collection to the ones in the incoming set that are different.</li>
</ul>

<p>The first two steps compare the Model's <code>resource_uri</code>s and the last part is done with SHA1 hashes.</p>

<pre>
window.Model = Backbone.Model.extend({
    urlRoot: BASE_API + 'model/',
    defaults: {
        foo: ''
    },
    initialize: function() {
        console.log('Model-&gt;initialize()', this);
        this.bind('change', function(model) {
            console.log('Model-&gt;change()', model);
        });
    }
});
window.ModelCollection = Backbone.Collection.extend({
    model: Model,
    url: BASE_API + 'model/',
    initialize: function() {
        console.log('ModelCollection-&gt;initialize()', this);
        this.bind('add', function(model) {
            console.log('ModelCollection-&gt;add()', model);
        });
        this.bind('remove', function(model) {
            console.log('ModelCollection-&gt;remove()', model);
        });
    }
});

window.Root = Backbone.Model.extend({
    urlRoot: BASE_API + 'root/',
    defaults: {
        models: new ModelCollection()
    },
    parse: function(data) {
        var attrs = data &amp;&amp; data.objects &amp;&amp; ( _.isArray( data.objects ) ? data.objects[ 0 ] : data.objects ) || data;
        var model = this;
        incoming_model_uris = _.map(attrs.models, function(model) {
            return model.resource_uri;
        });
        existing_model_uris = this.get('models').map(function(model) {
            return model.get('resource_uri');
        });
        _.each(existing_model_uris, function(uri) {
            if(incoming_model_uris.indexOf(uri) == -1) {
                model.get('models').remove(model.get('models').get(uri));
            }
        });
        _.each(incoming_model_uris, function(uri) {
            if(existing_model_uris.indexOf(uri) == -1) {
                model.get('models').add(_.detect( attrs.models, function(model) { return model.resource_uri == uri; }));
            }
        });
        _.each(attrs.models, function(model) {
            if(Sha1.hash(JSON.stringify(model)) != Sha1.hash(JSON.stringify(model.get('models').get(model.resource_uri)))) {
                model.get('models').get(model.resource_uri).set(model);
            }
        });         

        delete attrs.models;        

        return attrs;
    },
    initialize: function() {
        _.bindAll(this, 'parse');
        this.fetch();
    }
});
</pre>

<p>
<strong>Update 3:</strong> looking at this afterwards, I'm not sure of the complete watertightness of the above. Perhaps the <a href="http://kilon.org/blog/2012/02/backbone-poller/">Backbone Poller</a> project would be be a better approach.
</up>

<p>
<strong>Update 2:</strong> In order to avoid borking on an empty response, do:
</p>

<pre>
var attrs = Backbone.Model.prototype.parse.call(this, data);
if(!attrs) return;
</pre>
    
<p>
<strong>Update:</strong> My Javascript-Fu is weak which made me not see the obvious. As suggested in <a href="http://documentcloud.github.com/backbone/#Model-extend">Backbone.js documentation</a>, you can call the parent's implementation like this:
</p>

<pre>
Backbone.Model.prototype.method.call(this, args);
</pre>

<p>
So, instead of unnecessarily copypasting behavior from <a href="https://github.com/PaulUithol/backbone-tastypie">Backbone-tastypie.js</a>, we can say: 
</p>

<pre>
var attrs = Backbone.Model.prototype.parse.call(this, data);
</pre>

<p>
&hellip;and still have Backbone-tastypie.js do it's parsing thing for us.
</p>

<p>
Ugh.
</p>

]]></body>
  </post>
  <post>
    <published>2011-08-07 18:36:46</published>
    <slug>working-ntp-conf-for-the-pool</slug>
    <title>Working ntp.conf for the Pool</title>
    <category>note</category>
    <tags>
      <tag>ntp-pool</tag>
      <tag>ntpd</tag>
    </tags>
    <body><![CDATA[<pre>
driftfile /var/lib/ntp/ntp.drift

statsdir /var/log/ntpstats/

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

# From http://support.ntp.org/bin/view/Servers/StratumTwoTimeServers
# each of these ISO: FR, Notify?: No
server ntp1.kamino.fr iburst
server ntp1.doowan.net iburst
server ntp.duckcorp.org iburst
server itsuki.fkraiem.org iburst
server time.zeroloop.net iburst

restrict 127.0.0.1
restrict ::1
restrict default kod notrap nomodify nopeer noquery
</pre>

<p>
Pool stats for the servers be found <a href="http://www.pool.ntp.org/user/async-fi">here</a>.
</p>
]]></body>
  </post>
  <post>
    <published>2011-08-15 17:49:36</published>
    <slug>site-optimizations</slug>
    <title>Site optimizations</title>
    <category>meta</category>
    <tags>
      <tag>cdn</tag>
      <tag>cloudfront</tag>
      <tag>minify</tag>
      <tag>super-cache</tag>
    </tags>
    <body><![CDATA[<a href="/media/2011/08/20110815-cy3h5i9mxn5guuaqp4x39adtkf.png"><img src="/media/2011/08/20110815-cy3h5i9mxn5guuaqp4x39adtkf.png" alt="" title="Async.fi Pingdom 2011-08-12 - 2011-08-15" width="700" height="462" class="alignleft size-full wp-image-881" /></a>

<p>
Performace-wise, setting up <a href="http://aws.amazon.com/cloudfront/">Amazon CloudFront</a> (<a href="http://docs.amazonwebservices.com/AmazonCloudFront/latest/DeveloperGuide/index.html?CustomOriginBestPractices.html">"Custom Origin"</a>) in addition to <a href="http://wordpress.org/extend/plugins/wp-minify/">WP Minify</a> and <a href="http://wordpress.org/extend/plugins/wp-super-cache/">WP Super Cache</a> improved site response times a lot. Offloading static content to Amazon not only made those offloaded files load faster (because of Amazon's faster tubes) but this also reduced stress on our feeble-ish server on page load so that the document itself is returned faster. Load time is also more repeatable. Good stuff!
</p>

<p>
Note that CloudFront makes HTTP/1.0 requests and Apache may take some convincing in order to make it Gzip the 1.0 response.  
</p>
]]></body>
  </post>
  <post>
    <published>2011-08-26 10:26:50</published>
    <slug>debian-ssd-tips</slug>
    <title>Debian SSD tips</title>
    <category>note</category>
    <tags>
      <tag>debian</tag>
      <tag>grub</tag>
      <tag>scheduler</tag>
      <tag>ssd</tag>
    </tags>
    <body><![CDATA[
    <ul>
    <li>Do not use swap; this may be overly cautious these days as the drives have fancy wear-leveling schemes and whatnot implemented but still, if you're not tight on memory then it should not hurt. And if memory is an issue then in order to avoid performance problems perhaps you should upgrade it in the first place.</li>
    <li>Do use the "noop" I/O scheduler, i.e.</li>
    <li><code>apt-get install grub</code></li>
    <li>add <code>GRUB_CMDLINE_LINUX="elevator=noop"</code> to <code>/etc/default/grub</code></li>
    <li><code>update-grub</code></li>
    <li>after boot,&#160;<code>/sys/block/sda/queue/scheduler</code> should read "<code>[noop] anticipatory deadline cfq</code>"</li>
    </ul>
<div>That last part about the scheduler ensures that the default disk I/O scheduling, which rearranges reads and writes to boost IOPS for traditional cylindrical platters and is therefore just bad for SSD performance, is not used. With the "noop" scheduler, reads and writes happen in order.</div>]]></body>
  </post>
  <post>
    <published>2011-08-31 19:49:42</published>
    <slug>kinectvision-com-code</slug>
    <title>KinectVision.com code</title>
    <category>snippet</category>
    <tags>
      <tag>bash</tag>
      <tag>c</tag>
      <tag>kinect</tag>
      <tag>kinectvision-com</tag>
      <tag>make</tag>
      <tag>openkinect</tag>
    </tags>
    <body><![CDATA[
    <p>This is old, from December 2010 it seems, but it's here in case the machine goes titsup. Quick, dirty and ugly but it works most of the time. First, the capture program:</p>

<pre>
#include &lt;libusb-1.0/libusb.h&gt;
#include &quot;libfreenect.h&quot;
#include &quot;libfreenect_sync.h&quot;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

/*
  No error checking performed whatsoever; dealing with it later (or not).
 */
int main(int argc, char** argv)
{
  uint16_t * depth = (uint16_t *)malloc(FREENECT_DEPTH_11BIT_SIZE);
  uint32_t timestamp;
  int index = 0;
  freenect_depth_format fmt = FREENECT_DEPTH_11BIT;

  uint8_t * depth8 = (uint8_t *)malloc(FREENECT_FRAME_PIX);
  int i;

  /* Capture one Kinect depth frame */
  freenect_sync_get_depth(&amp;depth, &amp;timestamp, index, fmt);

  /* Convert captured frame to an 8-bit greyscale image */
  for(i = 0; i &lt; FREENECT_FRAME_PIX; i++) {
    depth8[i] = (2048 * 256) / (2048.0 - depth[i]);
  }

  /* Write raw greyscale image to stdout  */
  fwrite(depth8, FREENECT_FRAME_PIX, 1, stdout);

  return 0;
}
</pre>


<p>Makefile:</p>

<pre>
all:		capkinect

clean:
		rm -f capkinect.o capkinect

capkinect.o:	capkinect.c
	gcc -g -I/usr/local/include/libfreenect/ -c capkinect.c -o capkinect.o

capkinect:	capkinect.o
	gcc -g capkinect.o -L/usr/local/lib/ -lfreenect_sync -o capkinect
</pre>


<p>Uploader:</p>

<pre>
#!/bin/sh

INPUT=`mktemp`
AVG=`mktemp`
TEMP=`mktemp`
OUTPUT=`mktemp --directory`

#COLORMAP=&quot;black-#45931c&quot;
COLORMAP=&quot;black-white&quot;

# initial average frame
capkinect | rawtopgm 640 480 | pnmcut 8 8 624 464 | pgmtoppm $COLORMAP &gt;$AVG

while [ true ]; do

    #echo &quot;input: $INPUT avg: $AVG temp: $TEMP output: $OUTPUT colormap: $COLORMAP&quot;

    capkinect | rawtopgm 640 480 | pnmcut 8 8 624 464 | pgmtoppm $COLORMAP &gt;$INPUT

    FILENAME=$OUTPUT/`date +%s.%N`

    ppmmix 0.035 $AVG $INPUT &gt;$FILENAME.ppm

    cp $FILENAME.ppm $AVG

    cat $FILENAME.ppm | cjpeg -greyscale -quality 65 &gt;$FILENAME.jpg

    echo &quot;user=XXXX:AAAA&quot; | curl --digest -K - -F &quot;file=@$FILENAME.jpg&quot; http://kinectvision.com/depth

    rm $FILENAME.ppm $FILENAME.jpg

    sleep 1

done
</pre>

<p>Server end script that inputs and outputs frames:</p>

<pre>
$latest_path = $_SERVER[&quot;DOCUMENT_ROOT&quot;] . &quot;/incoming/latest&quot;;

if($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) {

  if(!isset($_FILES[&quot;file&quot;][&quot;name&quot;])) {
    exit();
  }
  if(move_uploaded_file($_FILES[&quot;file&quot;][&quot;tmp_name&quot;], $_SERVER[&quot;DOCUMENT_ROOT&quot;] . &quot;/incoming/&quot; . $_FILES[&quot;file&quot;][&quot;name&quot;])) {
    file_put_contents($latest_path, $_FILES[&quot;file&quot;][&quot;name&quot;]);
  }

} elseif($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;HEAD&quot;) {

  $latest = file_get_contents($latest_path);
  header(&quot;X-KinectVision-Latest: &quot; . $latest);

} elseif($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;GET&quot;) {

  $latest = $_SERVER[&quot;DOCUMENT_ROOT&quot;] . &quot;/incoming/&quot; . file_get_contents($latest_path);
  header(&quot;Content-Type: image/jpeg&quot;);
  header(&quot;X-KinectVision-Latest: &quot; . $latest);

  if(isset($_GET[&quot;width&quot;]) &amp;&amp; intval($_GET[&quot;width&quot;]) &lt; 624) {
    $width = intval($_GET[&quot;width&quot;]);
    $f = popen(&quot;djpeg -pnm -fast -greyscale $latest | pnmscalefixed -width=$width | cjpeg -greyscale -quality 65&quot;, &quot;r&quot;);
    while(!feof($f)) {
      echo fread($f, 1024);
    }
    fclose($f);
  } else {
    echo file_get_contents($latest);
  }

}
</pre>

<p>(Don't know if it's the <a href="http://aquoid.com/news/themes/suffusion/">Suffusion theme</a> or what that kill all the newlines from these listings. They're there, I can assure you, they're just not visible.)</p>]]></body>
  </post>
  <post>
    <published>2011-10-16 12:50:44</published>
    <slug>s3-client-upload-parameter-generation-with-python</slug>
    <title>S3 client-upload parameter generation with Python</title>
    <category>snippet</category>
    <tags>
      <tag>aws</tag>
      <tag>django</tag>
      <tag>http</tag>
      <tag>post</tag>
      <tag>python</tag>
      <tag>s3</tag>
    </tags>
    <body><![CDATA[<pre>
# http://aws.amazon.com/articles/1434   
def S3UploadParams(bucket_name, object_name, expiry, maxsize, redirect):
    import os, boto, json, base64, hmac, hashlib
    from time import time, gmtime, strftime

    def SignS3Upload(policy_document):
        policy = base64.b64encode(policy_document)
        return base64.b64encode(hmac.new(
                boto.config.get('Credentials', 'aws_secret_access_key'),
                policy,
                hashlib.sha1
                ).digest())

    def GenerateS3PolicyString(bucket_name, object_name, expiry, maxsize, redirect):
        policy_template = '{ &quot;expiration&quot;: &quot;%s&quot;, &quot;conditions&quot;: [ {&quot;bucket&quot;: &quot;%s&quot;}, [&quot;eq&quot;, &quot;$key&quot;, &quot;%s&quot;], {&quot;acl&quot;: &quot;private&quot;}, {&quot;success_action_redirect&quot;: &quot;%s&quot;}, [&quot;content-length-range&quot;, 0, %s] ] }'
        return policy_template % (
            strftime(&quot;%Y-%m-%dT%H:%M:%SZ&quot;, gmtime(time() + expiry)),
            bucket_name,
            object_name,
            redirect,
            maxsize
            )
    
    params = {
        'key': object_name,
        'AWSAccessKeyId': boto.config.get('Credentials', 'aws_access_key_id'),
        'acl': 'private',
        'success_action_redirect': redirect,
        }

    policy = GenerateS3PolicyString(bucket_name, object_name, expiry, maxsize, redirect)
    params['policy'] = base64.b64encode(policy)

    signature = SignS3Upload(policy)
    params['signature'] = signature

    return params
</pre>
]]></body>
  </post>
  <post>
    <published>2011-10-21 17:59:18</published>
    <slug>gone-cloudflare</slug>
    <title>Gone CloudFlare</title>
    <category>shoutout</category>
    <tags>
      <tag>cdn</tag>
      <tag>cloudflare</tag>
    </tags>
    <body><![CDATA[
    <p><a href="/media/2011/10/cloudflare-logo.png"><img class="alignleft size-full" title="cloudflare-logo" src="/media/2011/10/cloudflare-logo.png" alt="" /></a></p>

<p>
Enabled <a href="https://www.cloudflare.com/">CloudFlare</a> on this site, with nearly every optimization thing they offer. So far it's looking good, with an empty (browser) cache it takes a moment to load initial resources but after that subsequent page loads are <em>near-instantaneous</em> (click around to try this out). To get SSL properly working you have to get a Pro account. Recommended!
</p>

<p>
<strong>Update:</strong> Getting fishy numbers with Pingdom (over 2000 ms), although page load times from my own machines are ok (around 500 ms or so). Investigating&hellip;
</p>
]]></body>
  </post>
  <post>
    <published>2011-10-22 12:23:40</published>
    <slug>sns-verify-sh</slug>
    <title>sns-verify.sh</title>
    <category>snippet</category>
    <tags>
      <tag>aws</tag>
      <tag>bash</tag>
      <tag>openssl</tag>
      <tag>sns</tag>
    </tags>
    <body><![CDATA[
    <pre>
#!/bin/sh

if [ $# -lt 3 ]; then
  echo &quot;usage: sns-verify.sh CERT SIG MESS&quot;
  exit 1
fi

CERT=$1
SIG=$2
MESS=$3

PUB=`/bin/tempfile`
SIGRAW=`/bin/tempfile`

# http://sns-public-resources.s3.amazonaws.com/SNS_Message_Signing_Release_Note_Jan_25_2011.pdf
/usr/bin/openssl x509 -in $CERT -pubkey -noout &gt; $PUB
/usr/bin/base64 -i -d $SIG &gt; $SIGRAW
RET=`/usr/bin/openssl dgst -sha1 -verify $PUB -signature $SIGRAW $MESS`

if [ X&quot;$RET&quot; = X&quot;Verified OK&quot; ]; then
  exit 0
fi

exit 1
</pre>]]></body>
  </post>
  <post>
    <published>2011-10-23 12:49:30</published>
    <slug>handling-amazon-sns-notifications-with-a-tastypie-resource</slug>
    <title>Handling Amazon SNS notifications with a Tastypie Resource</title>
    <category>snippet</category>
    <tags>
      <tag>aws</tag>
      <tag>django</tag>
      <tag>python</tag>
      <tag>rest</tag>
      <tag>sns</tag>
      <tag>sqs</tag>
      <tag>tastypie</tag>
    </tags>
    <body><![CDATA[
    <p>Using <a href="https://www.djangoproject.com/">Django</a> and <a href="https://github.com/toastdriven/django-tastypie">Tastypie</a>, we automagically respond to <a href="http://aws.amazon.com/sns/">SNS</a> subscription requests. After that part is handled, the notification messages start coming in and those are used to trigger an <a href="http://aws.amazon.com/sqs/">SQS</a> polling cycle (trying to do a thorough job there which may seem like an overkill but it's not). A received SQS message is parsed and contents are passed to an external program that forks and exits which keeps the request from blocking.</p>

<pre>
from django.conf import settings
from tastypie import fields, http
from tastypie.resources import Resource
from tastypie.bundle import Bundle
from tastypie.authentication import Authentication
from tastypie.authorization import Authorization
from tastypie.throttle import BaseThrottle
import boto.sq 
from boto.sqs.message import Message
from urlparse import urlparse
import base64, httplib, tempfile, subprocess, time, json, os, sys, syslog

# Http://django-tastypie.readthedocs.org/en/latest/non_orm_data_sources.html
class NotificationObject(object):
    def __init__(self, initial=None):
        self.__dict__['_data'] = {}
        if hasattr(initial, 'items'):
            self.__dict__['_data'] = initial
    def __getattr__(self, name):
        return self._data.get(name, None)
    def __setattr__(self, name, value):
        self.__dict__['_data'][name] = value

class NotificationResource(Resource):
    sns_messageid = fields.CharField(attribute='MessageId')
    sns_timestamp = fields.CharField(attribute='Timestamp')
    sns_topicarn = fields.CharField(attribute='TopicArn')
    sns_type = fields.CharField(attribute='Type')
    sns_unsubscribeurl = fields.CharField(attribute='UnsubscribeURL')
    sns_subscribeurl = fields.CharField(attribute='SubscribeURL')
    sns_token = fields.CharField(attribute='Token')
    sns_message = fields.CharField(attribute='Message')
    sns_subject = fields.CharField(attribute='Subject')
    sns_signature = fields.CharField(attribute='Signature')
    sns_signatureversion = fields.CharField(attribute='SignatureVersion')
    sns_signingcerturl = fields.CharField(attribute='SigningCertURL')

    class Meta:
        resource_name = 'notification'
        object_class = NotificationObject
        fields = ['sns_messageid']
        list_allowed_methods = ['post']
        authentication = Authentication()
        authorization = Authorization()

    def get_resource_uri(self, bundle_or_obj):
        return ''

    def obj_create(self, bundle, request=None, **kwargs):

        bundle.obj = NotificationObject(initial={ 'MessageId': '', 'Timestamp': '', 'TopicArn': '', 'Type': '', 'UnsubscribeURL': '', 'SubscribeURL': '', 'Token': '', 'Message': '', 'Subject': '', 'Signature': '', 'SignatureVersion': '', 'SigningCertURL': '' })
        bundle = self.full_hydrate(bundle)

        o = urlparse(bundle.data['SigningCertURL'])
        if not o.hostname.endswith('.amazonaws.com'):
            return bundle

        topicarn = bundle.data['TopicArn']

        if topicarn != settings.SNS_TOPIC:
            return bundle

        if not self.verify_message(bundle):
            return bundle

        if bundle.data['Type'] == 'SubscriptionConfirmation':
            self.process_subscription(bundle)
        elif bundle.data['Type'] == 'Notification':
            self.process_notification(bundle)

        return bundle

    def process_subscription(self, bundle):
        syslog.syslog('SNS Subscription ' + bundle.data['SubscribeURL'])
        o = urlparse(bundle.data['SubscribeURL'])
        conn = httplib.HTTPSConnection(o.hostname)
        conn.putrequest('GET', o.path + '?' + o.query)
        conn.endheaders()
        response = conn.getresponse()
        subscription = response.read()

    def process_notification(self, bundle):
        sqs = boto.sqs.connect_to_region(settings.SQS_REGION)
        queue = sqs.lookup(settings.SQS_QUEUE)
        retries = 5
        done = False
        while True:
            if retries &lt; 1:
                break
            retries -= 1
            time.sleep(5)
            messages = queue.get_messages(10, visibility_timeout=60)
            if len(messages) &lt; 1:
                continue
            for message in messages:
                try:
                    m = json.loads(message.get_body())
                    m['return_sns_region'] = settings.SNS_REGION
                    m['return_sns_topic'] = settings.SNS_TOPIC
                    m['return_sqs_region'] = settings.SQS_REGION
                    m['return_sqs_queue'] = settings.SQS_QUEUE
                    process = subprocess.Popen(['/usr/bin/nice', '-n', '15', os.path.dirname(os.path.normpath(os.sys.modules[settings.SETTINGS_MODULE].__file__)) + '/process.py', base64.b64encode(json.dumps(m))], shell=False)
                    process.wait()
                except:
                    e = sys.exc_info()[1]
                    syslog.syslog(str(e))
                queue.delete_message(message)

    def verify_message(self, bundle):
        message = u''
        if bundle.data['Type'] == 'SubscriptionConfirmation':
            message += 'Message\n'
            message += bundle.data['Message'] + '\n'
            message += 'MessageId\n'
            message += bundle.data['MessageId'] + '\n'
            message += 'SubscribeURL\n'
            message += bundle.data['SubscribeURL'] + '\n'
            message += 'Timestamp\n'
            message += bundle.data['Timestamp'] + '\n'
            message += 'Token\n'
            message += bundle.data['Token'] + '\n'
            message += 'TopicArn\n'
            message += bundle.data['TopicArn'] + '\n'
            message += 'Type\n'
            message += bundle.data['Type'] + '\n'
        elif bundle.data['Type'] == 'Notification':
            message += 'Message\n'
            message += bundle.data['Message'] + '\n'
            message += 'MessageId\n'
            message += bundle.data['MessageId'] + '\n'
            if bundle.data['Subject'] != '':
                message += 'Subject\n'
                message += bundle.data['Subject'] + '\n'
            message += 'Timestamp\n'
            message += bundle.data['Timestamp'] + '\n'
            message += 'TopicArn\n'
            message += bundle.data['TopicArn'] + '\n'
            message += 'Type\n'
            message += bundle.data['Type'] + '\n'
        else:
            return False

        o = urlparse(bundle.data['SigningCertURL'])
        conn = httplib.HTTPSConnection(o.hostname)
        conn.putrequest('GET', o.path)
        conn.endheaders()
        response = conn.getresponse()
        cert = response.read()

        # ok; attempt to use m2crypto failed, using openssl command line tool instead

        file_cert = tempfile.NamedTemporaryFile(mode='w', delete=False)
        file_sig = tempfile.NamedTemporaryFile(mode='w', delete=False)
        file_mess = tempfile.NamedTemporaryFile(mode='w', delete=False)

        file_cert.write(cert)
        file_sig.write(bundle.data['Signature'])
        file_mess.write(message)

        file_cert.close()
        file_sig.close()
        file_mess.close()

        # see: https://async.fi/2011/10/sns-verify-sh/
        verify_process = subprocess.Popen(['/usr/local/bin/sns-verify.sh', file_cert.name, file_sig.name, file_mess.name], shell=False)
        verify_process.wait()

        if verify_process.returncode == 0:
            return True

        return False
</pre>

<p>That <code>process.py</code> would be something like:</p>

<pre>
#!/usr/bin/env python

import boto.sqs
from boto.sqs.message import Message
import base64, json, os, sys, syslog

if len(sys.argv) != 2:
    sys.exit('usage: %s &lt;base64 encoded json object&gt;' % (sys.argv[0], ))

m = json.loads(base64.b64decode(sys.argv[1]))

# http://code.activestate.com/recipes/66012-fork-a-daemon-process-on-unix/
try:
    pid = os.fork()
    if pid &gt; 0:
        sys.exit(0)
except OSError, e:
    print &gt;&gt;sys.stderr, &quot;fork #1 failed: %d (%s)&quot; % (e.errno, e.strerror)
    sys.exit(1)

os.chdir(&quot;/&quot;)
os.setsid()
os.umask(0)

try:
    pid = os.fork()
    if pid &gt; 0:
        sys.exit(0)
except OSError, e:
    sys.exit(1)

syslog.syslog(sys.argv[0] + ': ' + str(m))

# ...
</pre>

<p>That is, <code>process.py</code> gets the received (and doped) SQS message, Base64 encoded, as it's only command line argument, forks, exits and does what it's supposed to do after that on its own. Control returns to <code>NotificationResource</code> so the request doesn't block unnecessarily.</p>]]></body>
  </post>
  <post>
    <published>2011-12-12 09:25:47</published>
    <slug>note-to-self-more-on-south</slug>
    <title>Note to Self: More on South</title>
    <category>snippet</category>
    <tags>
      <tag>django</tag>
      <tag>south</tag>
    </tags>
    <body><![CDATA[
    <p>Setup being along the&#160;<em>dev</em>&#160;&#8594; <em>test</em>&#160;&#8594; <em>prod</em>&#160;lines,&#160;to correctly manage database migrations we first set things up at&#160;<em>dev</em>:</p>
<pre>manage.py syncdb --noinput
manage.py convert_to_south &lt;app&gt;
manage.py createsuperuser</pre>
<p>At this point the South migrations are being pushed to repository and pulled in at&#160;<em>test</em>:</p>
<pre>manage.py syncdb --noinput
manage.py migrate
manage.py migrate &lt;app&gt; 0001 --fake
manage.py createsuperuser</pre>
<p>Now, back at <em>dev</em>, after a change to one of the models:</p>
<pre>manage.py schemamigration &lt;app&gt; --auto
manage.py migrate &lt;app&gt;</pre>
And, after push/pull, at <em>test</em>:
<pre>manage.py migrate &lt;app&gt;</pre>]]></body>
  </post>
  <post>
    <published>2012-01-29 10:55:52</published>
    <slug>mikrotik-openvpn-server</slug>
    <title>Mikrotik OpenVPN Server </title>
    <category>experimental</category>
    <tags>
      <tag>aws</tag>
      <tag>debian</tag>
      <tag>ec2</tag>
      <tag>mikrotik</tag>
      <tag>openvpn</tag>
      <tag>rb750</tag>
      <tag>rdp</tag>
      <tag>smb</tag>
      <tag>windows-server-2008-r2</tag>
    </tags>
    <body><![CDATA[<p>The purpose of this post is to describe, step by step, my attempt to set up an <a href="http://en.wikipedia.org/wiki/OpenVPN">OpenVPN</a> server on a <a href="http://routerboard.com/RB750">Mikrotik RouterBOARD 750</a> and create a working tunnel from an outside machine (<a href="http://aws.amazon.com/windows/">AWS EC2 Windows Server 2008 R2</a>) to this OpenVPN server so that an <a href="http://en.wikipedia.org/wiki/Samba_(software)">SMB server</a> on the local network can be accessed from said outside machine. The following diagram gives an overview of the setup:</p>

<p><a href="/media/2012/01/MikrotikOpenVPNTest1.png"><img class="alignleft size-full" title="Mikrotik OpenVPN Server Test" src="/media/2012/01/MikrotikOpenVPNTest1.png" alt="" /></a></p>

<p>I am going to decribe how to:</p>
<ul>
	<li>generate certificates to be used with OpenVPN</li>
	<li>set up OpenVPN server on Mikrotik router</li>
	<li>set up a tunnel with <a href="http://www.openvpn.net/index.php/open-source/downloads.html">OpenVPN client</a> on Windows</li>
</ul>
I am not going to describe the following:
<ul>
	<li>setting up and connecting to an EC2 Windows instance</li>
	<li>setting up a Samba Server</li>
</ul>
A few things worth mentioning about Mikrotik OpenVPN server implementation (that will likely bite if not known in advance):
<ul>
	<li>only supports TCP mode, UDP is not supported</li>
	<li>username/password pair is also required even though certificates are being used for authentication</li>
</ul>

<h3>Generate certificates to be used with OpenVPN</h3>
<pre>root@inhouse-debian:~# <strong>apt-get install openvpn</strong>
root@inhouse-debian:~# <strong>mkdir ovpn-cert</strong>
root@inhouse-debian:~# <strong>cd ovpn-cert/</strong>
root@inhouse-debian:~/ovpn-cert# <strong>cp -r /usr/share/doc/openvpn/examples/easy-rsa/2.0/* .</strong>
root@inhouse-debian:~/ovpn-cert# <strong>emacs vars</strong></pre>
In the file <code>vars</code> I set the following values:
<pre>export KEY_COUNTRY="FI"
export KEY_PROVINCE="Etela-Suomi"
export KEY_CITY="Kotka"
export KEY_ORG="Async.fi"
export KEY_EMAIL="joni.kahara@async.fi"
export KEY_CN="kahara.dyndns.org"
export KEY_NAME="kahara.dyndns.org"
export KEY_OU="kahara.dyndns.org"</pre>
If I have understood correctly, of these only <em>CN</em> (Common Name) is obligatory. I may be wrong. Anyway, continuing:
<pre>root@inhouse-debian:~/ovpn-cert# <strong>source vars</strong>
root@inhouse-debian:~/ovpn-cert# <strong>./clean-all</strong>
root@inhouse-debian:~/ovpn-cert# <strong>./build-ca</strong>
root@inhouse-debian:~/ovpn-cert# <strong>./build-key-server kahara.dyndns.org</strong>
root@inhouse-debian:~/ovpn-cert# <strong>openssl rsa -in keys/kahara.dyndns.org.key -out keys/kahara.dyndns.org.pem</strong>
root@inhouse-debian:~/ovpn-cert# <strong>./build-key ec2 </strong>
root@inhouse-debian:~/ovpn-cert# <strong>apt-get install ncftp</strong>
root@inhouse-debian:~/ovpn-cert# <strong>ncftpput -u admin 192.168.1.1 / keys/kahara.dyndns.org.crt keys/kahara.dyndns.org.pem keys/ca.crt</strong></pre>
<h3>Set up OpenVPN server on Mikrotik router</h3>
All the stuff here can also be made through Mikrotik's admin interface; textual form without screen shots is used just to keep thing terse.
<pre>root@inhouse-debian:~/ovpn-cert# <strong>ssh admin@192.168.1.1</strong>
[admin@MikroTik] &gt; <strong>/certificate</strong>
[admin@MikroTik] /certificate&gt; <strong>import file=kahara.dyndns.org.crt</strong>
[admin@MikroTik] /certificate&gt; <strong>import file=kahara.dyndns.org.pem</strong>
[admin@MikroTik] /certificate&gt; <strong>import file=ca.crt</strong>
[admin@MikroTik] /certificate&gt; <strong>decrypt</strong>
[admin@MikroTik] /certificate&gt; <strong>..</strong>
[admin@MikroTik] &gt;<strong> /interface bridge add name=ovpn-bridge</strong>
[admin@MikroTik] &gt;<strong> /interface bridge port add interface=ether2-master-local bridge=ovpn-bridge</strong>
[admin@MikroTik] &gt;<strong> /ip address add address=192.168.1.64/24 interface=ovpn-bridge </strong>
[admin@MikroTik] &gt;<strong> /ip pool add name=ovpn-pool ranges=192.168.1.65-192.168.1.99</strong>
[admin@MikroTik] &gt; <strong>/ppp profile add bridge=ovpn-bridge name=ovpn-profile remote-address=ovpn-pool</strong>
[admin@MikroTik] &gt; <strong>/ppp secret add service=ovpn local-address=192.168.1.64 name=user1 password=pass1 profile=ovpn-profile</strong>
[admin@MikroTik] &gt; <strong>/interface ovpn-server server set auth=sha1,md5 certificate=cert1 cipher=blowfish128,aes128,aes192,aes256 default-profile=ovpn-profile enabled=yes keepalive-timeout=disabled max-mtu=1500 mode=ethernet netmask=24 port=1194 require-client-certificate=yes</strong>
[admin@MikroTik] &gt; <strong>/ip firewall filter add action=accept chain=input disabled=no protocol=tcp dst-port=1194</strong>
[admin@MikroTik] &gt; <strong>/ip firewall filter move 5 destination=1</strong></pre>
That last step moves the new rule to the front of the chain; numbers ("5", "1") will likely be something else on your configuration. Firewall rule listing can be printed with the following command:
<pre>[admin@MikroTik] &gt; <strong>/ip firewall filter print</strong></pre>
<h3>Setup up a tunnel with OpenVPN client on Windows</h3>
After installing OpenVPN, create a config file for it. Here it's called "kahara.dyndns.org.ovpn":
<pre>client
dev tap
proto tcp
remote kahara.dyndns.org 1194
resolv-retry infinite
nobind
persist-key
persist-tun
ca ca.crt
cert ec2.crt
key ec2.key
verb 3
pull
auth-user-pass userpass.txt</pre>
Also, create a file called "userpass.txt" and put the following to it:
<pre>user1
pass1</pre>
Of course in an IRL situation one should use a real password. Make sure you copied the .crt and .key files over to the Windows machine, after which you can run OpenVPN client with:
<pre>PS C:\Users\Administrator\Desktop&gt; <strong>openvpn.exe .\kahara.dyndns.org.ovpn</strong></pre>
And here we have an EC2 client connected to a local SMB resource over the tunnel:

<a href="/media/2012/01/ec2-46-137-68-14.eu-west-1.compute.amazonaws.com_.png"><img class="alignleft size-full" title="ec2-46-137-68-14.eu-west-1.compute.amazonaws.com" src="/media/2012/01/ec2-46-137-68-14.eu-west-1.compute.amazonaws.com_.png" alt="" /></a>]]></body>
  </post>
  <post>
    <published>2012-02-11 09:24:15</published>
    <slug>daily-mysql-database-dump-back-up-to-s3</slug>
    <title>Daily MySQL database dump, back up to S3</title>
    <category>snippet</category>
    <tags>
      <tag>mysql</tag>
      <tag>mysqldump</tag>
      <tag>s3</tag>
      <tag>s3cmd</tag>
    </tags>
    <body><![CDATA[<p>I'm in the process of, or planning at least, ditching MySQL/WordPress/CloudFlare and moving to a static site hosted on S3/CloudFront. At the moment, as AWS Route 53 does not support S3 or CloudFront as an Alias Target, moving to S3/CloudFront means that I have to have an A record pointing to a web server somewhere, which in turn redirects the request to the actual site's CloudFront CNAME. I do have such a server (running Nginx), but the same thing could be as well achieved by using a service such as <a href="http://www.arecord.net/">Arecord.net</a>. This redirect means that there's no way to run a site without the www.-prefix. Which I can live with. Also, at the moment, no SSL support is available but I'm sure I can live with that too as WordPress is simply slow, and most of all a big waste of resources. Getting rid of all the dynamic parts (seriously, it's not like there are a lot of commenters around here) will make this thing run fast, at least compared to what page load times currently are. My tests show that CloudFront returns cached pages in less than 200ms.</p>

<p>So, I'm killing one extra server in the near future and putting these snippets here for my own possible future use.</p>

<em>~/.my.cnf:</em>
<pre>[client]
user = <strong>usename</strong>
password = <strong>password</strong>
host = <strong>hostname</strong>

[mysql]
database = <strong>dbname </strong></pre>
<em>&lt;dir&gt;/wp-db-backup.sh:</em>
<pre>#!/bin/sh

DBFILE="&lt;dir&gt;/<strong>dbname</strong>-`/bin/date +%s`.gz"

/usr/bin/mysqldump --quick <strong>dbname</strong> | /bin/gzip -c &gt;$DBFILE
/usr/bin/s3cmd put $DBFILE s3://<strong>bucketname</strong>/
/bin/rm $DBFILE</pre>
<em>crontab:</em>
<pre>45 3 * * * /usr/bin/nice -n 20 &lt;dir&gt;/wp-db-backup.sh &gt;/dev/null 2&gt;&amp;1</pre>
&nbsp;]]></body>
  </post>
  
  <post>
    <published>2012-03-03 10:00:00</published>
    <slug>gone-static</slug>
    <title>Gone static</title>
    <category>meta</category>
    <tags>
      <tag>aws</tag>
      <tag>s3</tag>
      <tag>cloudfront</tag>
      <tag>mysql</tag>
      <tag>php</tag>
      <tag>wordpress</tag>
    </tags>
    <body><![CDATA[
    <p>
    <strong>Update 2:</strong> Looks like page load times, at least as reported by Pingdom, went up from what they initially were. In my own testing cached pages still load in something like 150 to 250 milliseconds but Pingdom disagees. I don't know if this is regular CloudFront performance fluctuation, some kind of impedance mismatch between Pingdom and CloudFront or "something else".
    </p>
    <p>
    <a href="/media/2012/03/Pingdom-2012-03-06.png"><img class="alignleft size-full" title="Pingdom" src="/media/2012/03/Pingdom-2012-03-06.png" alt="" /></a>
    </p>
    <p>
    <strong>Update:</strong> That really did the trick and the <a href="https://plus.google.com/112535968102790189432/posts/7trQ7LQdT36">estimated -90% page load time</a> wasn't that far off:
    </p>
    <p>
    <a href="/media/2012/03/Pingdom-2012-03-04.png"><img class="alignleft size-full" title="Pingdom" src="/media/2012/03/Pingdom-2012-03-04.png" alt="" /></a>
    </p>
    <p>
    Having been fed up with wastefulness (resource wise) and general slowness of the MySQL/PHP/WordPress/CloudFlare setup for some time, I have now moved this site to S3/CloudFront. Site is generated from an XML file (which I derived from a WordPress export dump) with a Python script that is hosted <a href="https://github.com/kahara/Async.fi">here</a>. Commenting is obviously impossible but if you for some reason need to contact me you'll find contact details on your left.
    </p>
    ]]></body>
  </post>
  
  <post>
    <published>2012-03-31 06:55:00</published>
    <slug>kettler-ergometer-serial-protocol</slug>
    <title>Kettler ergometer serial protocol</title>
    <category>note</category>
    <tags>
      <tag>kettler</tag>
      <tag>ergometer</tag>
      <tag>rs-232</tag>
    </tags>
    <body><![CDATA[
    <p>
    I'm having some issues getting <a href="http://trac.jergometer.org/">JErgometer</a> working with my new Kettler E3, so to rule out the possibility of miswiring (the bike has a traditional RS-232 interface so I got a <a href="http://www.ftdichip.com/Products/Cables/USBRS232.htm">USB-RS232-WE-5000-BT_0.0</a> cable from FTDI which comes "wire-ended" and requires some soldering) I dug out <a href="http://home.vianetworks.nl/users/mhwlng/kettler.lua">"Kettler Treadmill Serial lua class"</a> from <a href="http://home.vianetworks.nl/users/mhwlng/">mhwlng</a> (thanks!). This, and my own brute-force testing got me the following list of commands relevant in the bike's context, which may be only a subset of the commands that the bike accepts but what could be used to build one's own implementation:
    </p>
    
    <dl>
    <dt><code>RS</code></dt>
    <dl>Reset device</dl>
    
    <dt><code>ID</code></dt>
    <dl>Ergometer computer model info ("SF1B1706")</dl>
    
    <dt><code>VE</code></dt>
    <dl>Ergormeter computer firmware version ("117")</dl>

    <dt><code>ST</code></dt>
    <dl>Request status; reply:<br><br>
    <code>pulse rpm speed*10 distance requested_power energy mm:ss actual_power</code><br>
    </dl>
    
    <dt><code>CM</code></dt>
    <dl>Enter command mode; required before calling the P-commands below</dl>
    
    <dt><code>PW x</code></dt>
    <dl>Request power of x watts</dl>
    
    <dt><code>PT mmss</code></dt>
    <dl>Request time of mmss</dl>
    
    <dt><code>PD x</code></dt>
    <dl>Request x/10 km distance</dl>
    
    </dl>

    <p>
    Notes: I only connected RX, TX and signal ground (looks like the bike does not use any kind handshaking). Serial port settings are 9600bps, 8N1.
    </p>    
    ]]></body>
  </post>

  <post>
    <published>2012-04-14 12:00:00</published>
    <slug>debugging-pyergometer</slug>
    <title>Debugging pyergometer</title>
    <category>note</category>
    <tags>
      <tag>kettler</tag>
      <tag>ergometer</tag>
      <tag>pyergometer</tag>
    </tags>
    <body><![CDATA[
    <p>
    <a href="https://github.com/kahara/pyergometer">pyergometer</a>
    </p>
    
    <script type="text/javascript" src="//ajax.googleapis.com/ajax/static/modules/gviz/1.0/chart.js"> {"dataSourceUrl":"//docs.google.com/spreadsheet/tq?key=0AkGOC2pAlAfOdGZiWjI3Sk5ZRTlGTDFlaEk3TEp4YXc&transpose=0&headers=1&range=B1%3AE1201&gid=0&pub=1","options":{"series":{"3":{"lineWidth":1},"2":{"lineWidth":1},"1":{"lineWidth":1},"0":{"lineWidth":1}},"curveType":"","animation":{"duration":500},"theme":"maximized","lineWidth":2,"logScale":false,"hAxis":{"viewWindowMode":"pretty","viewWindow":{}},"vAxes":[{"viewWindowMode":"pretty","viewWindow":{}},{"viewWindowMode":"pretty","viewWindow":{}}],"title":"","booleanRole":"certainty","useFormatFromData":true,"legend":"in","focusTarget":"category","width":550,"height":350},"state":{},"view":{"columns":[{"calc":"emptyString","type":"string","sourceColumn":0},0,1,2,3]},"chartType":"LineChart","chartName":"Chart 1"} </script>    
    ]]></body>
  </post>
  
</posts>

